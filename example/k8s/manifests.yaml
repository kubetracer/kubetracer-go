apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: tempo-pv
  namespace: monitoring
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /var/lib/tempo
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: tempo-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: tempo-config
  namespace: monitoring
data:
  tempo.yaml: |
    stream_over_http_enabled: true
    server:
      http_listen_port: 3200
      log_level: info
    cache:
      background:
        writeback_goroutines: 5
      caches:
        - roles:
            - frontend-search
          memcached:
            addresses: memcached.monitoring.svc.cluster.local:11211
    query_frontend:
      search:
        duration_slo: 5s
        throughput_bytes_slo: 1.073741824e+09
        metadata_slo:
          duration_slo: 5s
          throughput_bytes_slo: 1.073741824e+09
      trace_by_id:
        duration_slo: 100ms
      metrics:
        max_duration: 120h
        query_backend_after: 5m
        duration_slo: 5s
        throughput_bytes_slo: 1.073741824e+09
    distributor:
      receivers:
        jaeger:
          protocols:
            thrift_http:
              endpoint: "0.0.0.0:14268"
            grpc:
              endpoint: "0.0.0.0:14250"
            thrift_binary:
              endpoint: "0.0.0.0:6832"
            thrift_compact:
              endpoint: "0.0.0.0:6831"
        zipkin:
          endpoint: "0.0.0.0:9411"
        otlp:
          protocols:
            grpc:
              endpoint: "0.0.0.0:4317"
            http:
              endpoint: "0.0.0.0:4318"
        opencensus:
          endpoint: "0.0.0.0:55678"
    ingester:
      max_block_duration: 5m
    compactor:
      compaction:
        block_retention: 24h
    metrics_generator:
      registry:
        external_labels:
          source: tempo
          cluster: kubernetes
      storage:
        path: /var/tempo/generator/wal
        remote_write:
          - url: http://prometheus.monitoring.svc.cluster.local:9090/api/v1/write
            send_exemplars: true
      traces_storage:
        path: /var/tempo/generator/traces
      processor:
        local_blocks:
          filter_server_spans: false
          flush_to_storage: true
    storage:
      trace:
        backend: local
        wal:
          path: /var/tempo/wal
        local:
          path: /var/tempo/blocks
    overrides:
      defaults:
        metrics_generator:
          processors: [service-graphs, span-metrics, local-blocks]
          generate_native_histograms: both
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: memcached
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: memcached
  template:
    metadata:
      labels:
        app: memcached
    spec:
      containers:
        - name: memcached
          image: memcached:1.6.29
          env:
            - name: MEMCACHED_MAX_MEMORY
              value: "64m"
            - name: MEMCACHED_THREADS
              value: "4"
          ports:
            - containerPort: 11211
---
apiVersion: v1
kind: Service
metadata:
  name: memcached
  namespace: monitoring
spec:
  ports:
    - port: 11211
      targetPort: 11211
  selector:
    app: memcached
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tempo
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: tempo
  template:
    metadata:
      labels:
        app: tempo
    spec:
      initContainers:
        - name: init-permissions
          image: busybox
          command: ["sh", "-c", "chown -R 10001:10001 /var/tempo"]
          volumeMounts:
            - name: tempo-storage
              mountPath: /var/tempo
      containers:
        - name: tempo
          image: grafana/tempo:latest
          args: ["-config.file=/etc/tempo.yaml"]
          volumeMounts:
            - name: tempo-config
              mountPath: /etc/tempo.yaml
              subPath: tempo.yaml
            - name: tempo-storage
              mountPath: /var/tempo
          ports:
            - containerPort: 14268
            - containerPort: 3200
            - containerPort: 9095
            - containerPort: 4317
            - containerPort: 4318
            - containerPort: 9411
      volumes:
        - name: tempo-config
          configMap:
            name: tempo-config
        - name: tempo-storage
          persistentVolumeClaim:
            claimName: tempo-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: tempo
  namespace: monitoring
spec:
  ports:
    - name: thrift-http
      port: 14268
      targetPort: 14268
    - name: http-metrics
      port: 3200
      targetPort: 3200
    - name: internal-metrics
      port: 9095
      targetPort: 9095
    - name: otlp-grpc
      port: 4317
      targetPort: 4317
    - name: otlp-http
      port: 4318
      targetPort: 4318
    - name: zipkin
      port: 9411
      targetPort: 9411
  selector:
    app: tempo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
        - name: prometheus
          image: prom/prometheus:latest
          args:
            - --config.file=/etc/prometheus.yaml
            - --web.enable-remote-write-receiver
            - --enable-feature=exemplar-storage
            - --enable-feature=native-histograms
          volumeMounts:
            - name: prometheus-config
              mountPath: /etc/prometheus.yaml
              subPath: prometheus.yaml
          ports:
            - containerPort: 9090
      volumes:
        - name: prometheus-config
          configMap:
            name: prometheus-config
---
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  ports:
    - port: 9090
      targetPort: 9090
  selector:
    app: prometheus
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
        - name: grafana
          image: grafana/grafana:11.2.0
          env:
            - name: GF_AUTH_ANONYMOUS_ENABLED
              value: "true"
            - name: GF_AUTH_ANONYMOUS_ORG_ROLE
              value: "Admin"
            - name: GF_AUTH_DISABLE_LOGIN_FORM
              value: "true"
            - name: GF_FEATURE_TOGGLES_ENABLE
              value: "traceqlEditor metricsSummary"
            - name: GF_INSTALL_PLUGINS
              value: "https://storage.googleapis.com/integration-artifacts/grafana-exploretraces-app/grafana-exploretraces-app-latest.zip;grafana-traces-app"
          volumeMounts:
            - name: grafana-datasources
              mountPath: /etc/grafana/provisioning/datasources/datasources.yaml
              subPath: datasources.yaml
          ports:
            - containerPort: 3000
      volumes:
        - name: grafana-datasources
          configMap:
            name: grafana-datasources
---
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  ports:
    - port: 3000
      targetPort: 3000
      nodePort: 32000
  selector:
    app: grafana
  type: NodePort
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s  # Default scrape interval, can be overridden by specific job settings
      evaluation_interval: 15s  # Default evaluation interval for rules
      external_labels:
        monitor: 'k8s-monitor'

    # Alertmanager configuration (if you have an Alertmanager setup)
    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              # - alertmanager.monitoring.svc.cluster.local:9093

    # Scraping configuration
    scrape_configs:
      # Scrape Prometheus itself
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']

      # Scrape Tempo
      - job_name: 'tempo'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace]
            action: keep
            regex: (tempo);monitoring

      # Scrape Grafana
      - job_name: 'grafana'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace]
            action: keep
            regex: (grafana);monitoring

      # Scrape Kubelet metrics
      - job_name: 'kubelet'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics

      # Scrape Kube-State-Metrics (if deployed)
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']

      # Scrape Node Exporter (if deployed)
      - job_name: 'node-exporter'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_namespace]
            action: keep
            regex: (node-exporter);monitoring

      # Scrape CAdvisor (if deployed)
      - job_name: 'cadvisor'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor

      # Scrape Kubernetes API server
      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: default;kubernetes;https

      # Scrape Kubernetes controller manager
      - job_name: 'kubernetes-controller-manager'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: kube-system;kube-controller-manager;https

      # Scrape Kubernetes scheduler
      - job_name: 'kubernetes-scheduler'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: endpoints
        relabel_configs:
          - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
            action: keep
            regex: kube-system;kube-scheduler;https

      # Scrape Kubernetes nodes
      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: monitoring
data:
  datasources.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        url: http://prometheus.monitoring.svc.cluster.local:9090
        isDefault: true
        jsonData:
          timeInterval: 10s
      - name: Tempo
        type: tempo
        access: proxy
        url: http://tempo.monitoring.svc.cluster.local:3200
        jsonData:
          httpHeaderName1: "X-Scope-OrgID"
          httpHeaderValue1: "1" 
---
apiVersion: apps/v1  
kind: Deployment  
metadata:  
  name: example-operator  
  namespace: monitoring  
spec:  
  replicas: 1  
  selector:  
    matchLabels:  
      app: example-operator  
  template:  
    metadata:  
      labels:  
        app: example-operator  
    spec:  
      serviceAccountName: example-operator-sa  
      containers:  
        - name: example-operator  
          image: localhost:5001/example-operator:latest  
          imagePullPolicy: Always  
          ports:  
            - containerPort: 8080  
          env:  
            - name: TEMPO_ENDPOINT  
              value: "tempo.monitoring.svc.cluster.local:4318" 
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: selfsigned-cluster-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: my-selfsigned-ca
  namespace: cert-manager
spec:
  isCA: true
  commonName: my-selfsigned-ca
  secretName: root-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: selfsigned-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: my-ca-issuer
spec:
  ca:
    secretName: root-secret
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: kubetracer-webhook-cert
  namespace: monitoring
spec:
  secretName: kubetracer-webhook-cert-tls
  duration: 24h
  renewBefore: 12h
  commonName: kubetracer-webhook.monitoring.svc
  dnsNames:
  - kubetracer-webhook.monitoring.svc
  issuerRef:
    name: my-ca-issuer
    kind: ClusterIssuer
    group: cert-manager.io